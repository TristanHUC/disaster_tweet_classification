{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade pip\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U datasets scipy ipywidgets matplotlib\n",
    "!pip install scikit-learn"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xZAoOjklFf6l",
    "outputId": "5126bd58-fa8c-416c-f786-4c840af99570",
    "ExecuteTime": {
     "end_time": "2024-05-08T18:20:44.691639Z",
     "start_time": "2024-05-08T18:15:58.835371Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages (24.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n",
      "^C\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages (1.4.1.post1)\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages (from scikit-learn) (1.12.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages (from scikit-learn) (1.3.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages (from scikit-learn) (3.4.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -ransformers (c:\\users\\trist\\appdata\\local\\pypoetry\\cache\\virtualenvs\\implementation-test-g2z_xs4m-py3.9\\lib\\site-packages)\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import set_seed\n",
    "import pandas as pd\n",
    "from utils import call_transformer\n",
    "from preprocessing import pre_processing\n",
    "from peft import LoraConfig, TaskType\n",
    "import os"
   ],
   "metadata": {
    "id": "5L7zNKPREy-v",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "c4393d7c-fa74-4647-8431-b6f4770e493e"
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "access_token = \"your access token to huggingface\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"HF_TOKEN\"] = access_token\n",
    "\n",
    "\n",
    "SEED = 123\n",
    "set_seed(SEED)\n",
    "\n",
    "R, LORA_ALPHA, LORA_DROPOUT = 64, 32, 0.1\n",
    "lora_conf = LoraConfig(\n",
    "    r=R,\n",
    "    lora_alpha=LORA_ALPHA,\n",
    "    lora_dropout=LORA_DROPOUT,\n",
    "    task_type=TaskType.SEQ_CLS,\n",
    "    target_modules='all-linear'\n",
    ")\n",
    "\n",
    "# list of models I have tested yet\n",
    "MODELS = {\n",
    "    \"BERT\": \"bert-base-cased\",\n",
    "    \"DEBERTA\": \"microsoft/deberta-base\",\n",
    "    \"ERNIE\": \"nghuyong/ernie-2.0-base-en\",\n",
    "    \"DEBERTAv3\": \"MoritzLaurer/DeBERTa-v3-large-mnli-fever-anli-ling-wanli\",\n",
    "    \"Bertweet\": \"vinai/bertweet-base\",\n",
    "    \"BERT_LARGE\": \"bert-large-cased\",\n",
    "    \"ERNIE_LARGE\": \"nghuyong/ernie-2.0-large-en\",\n",
    "    \"Bertweet_LARGE\": \"vinai/bertweet-large\",\n",
    "    \"MISTRAL_7B\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"Gemma_2B\": 'google/gemma-2b'\n",
    "}\n",
    "\n",
    "#retrieve the dataframe\n",
    "train_csv = pd.read_csv('train.csv', sep = ',', header=0)\n",
    "test_csv = pd.read_csv('test.csv', sep=',', header=0)\n",
    "\n",
    "#apply pre-processing\n",
    "train_dataframe, test_dataframe = pre_processing(train_csv, test_csv)\n",
    "\n",
    "\n",
    "\n",
    "call_transformer(MODELS,\"BERT\",train_dataframe, test_dataframe,training_batch_size=8,eval_batch_size=8,learning_rate=2e-5,num_train_epochs=1,weight_decay=0, lora_config= lora_conf, prediction = True , save = True, access_token = access_token)\n",
    "# accuracy : 0.839754816112084 accuracy on validation set\n",
    "\n",
    "\n",
    "#call_transformer(MODELS,\"DEBERTA\", train_dataframe, test_dataframe, training_batch_size=8, eval_batch_size=8, learning_rate=2e-5, num_train_epochs=10,weight_decay=0.01, lora_config= lora_conf, prediction = False , save = False, access_token = access_token)\n",
    "#accuracy :  0.8359580052493438 accuracy on validation set\n",
    "\n",
    "\n",
    "#call_transformer(MODELS,\"ERNIE\",train_dataframe, test_dataframe, training_batch_size=8, eval_batch_size=8,learning_rate=1e-5,num_train_epochs=4,weight_decay=0.01, lora_config= lora_conf, prediction = False , save = False, access_token = access_token)\n",
    "# 0.84251968503937 accuracy on validation set for 6 epochs\n",
    "\n",
    "#call_transformer(MODELS,\"DEBERTAv3\",train_dataframe, test_dataframe, training_batch_size=16,eval_batch_size=64,learning_rate=5e-06,num_train_epochs=4,weight_decay=0.01, lora_config= lora_conf, prediction = False , save = False, access_token = access_token)\n",
    "#0.8169877408056042 accuracy on validation set\n",
    "\n",
    "\n",
    "#call_transformer(MODELS,\"Bertweet\", train_dataframe, test_dataframe, training_batch_size=128, eval_batch_size=128, learning_rate=2e-5, num_train_epochs=4,weight_decay=0.01, lora_config= lora_conf, prediction = False , save = False, access_token = access_token)\n",
    "#accuracy :  0.8241469816272966 accuracy on validation set\n",
    "\n",
    "#call_transformer(MODELS,\"ERNIE_LARGE\",train_dataframe, test_dataframe, training_batch_size=8, eval_batch_size=8,learning_rate=2e-5,num_train_epochs=6,weight_decay=0.01, lora_config= lora_conf, prediction = False , save = False, access_token = access_token)\n",
    "\n",
    "#call_transformer(MODELS,\"BERT_LARGE\",train_dataframe, test_dataframe, training_batch_size=8,eval_batch_size=8,learning_rate=2e-5,num_train_epochs=4,weight_decay=0.01, lora_config= lora_conf, prediction = False , save = False, access_token = access_token)\n",
    "#bad accuracy\n",
    "\n",
    "#call_transformer(MODELS,\"Bertweet_LARGE\", train_dataframe, test_dataframe, training_batch_size=64, eval_batch_size=64, learning_rate=2e-5, num_train_epochs=1,weight_decay=0.01, prediction = False , save = False, access_token = access_token)\n",
    "#0.8293963254593176 accuracy on validation set\n",
    "\n",
    "#call_transformer(MODELS,\"Bertweet_LARGE\", train_dataframe, test_dataframe, training_batch_size=8, eval_batch_size=8, learning_rate=2e-5, num_train_epochs=1,weight_decay=0.01, lora_config= lora_conf, prediction = True , save = False, access_token = access_token)\n",
    "#checkout point 700 and 800 : 0.86 accuracy on validation set\n",
    "\n",
    "#call_transformer(MODELS,\"MISTRAL_7B\", train_dataframe, test_dataframe, training_batch_size=2, eval_batch_size=2, learning_rate=2e-5, num_train_epochs=3,weight_decay=0.01, lora_config= lora_conf, prediction = True , save = False, access_token = access_token)\n",
    "#84% accuracy\n",
    "\n",
    "#call_transformer(MODELS,\"Gemma_2B\", train_dataframe, test_dataframe, training_batch_size=8, eval_batch_size=8, learning_rate=2e-5, num_train_epochs=3,weight_decay=0.01, lora_config= lora_conf, prediction = True , save = False, access_token = access_token)\n",
    "#85% accuracy\n"
   ],
   "metadata": {
    "id": "Lu6eO7piBS8y",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 529
    },
    "outputId": "bc5884bd-c252-49f6-8fe7-9c9cd7c6be52"
   },
   "execution_count": 4,
   "outputs": [
    {
     "output_type": "error",
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-4-7068d4505c82>\u001B[0m in \u001B[0;36m<cell line: 100>\u001B[0;34m()\u001B[0m\n\u001B[1;32m     98\u001B[0m \u001B[0;31m#train_transformer(\"MISTRAL_7B\", train_dataset, training_batch_size=2, eval_batch_size=2, learning_rate=2e-5, num_train_epochs=2,weight_decay=0.01,disable_tqdm=False, lora_config= lora_conf)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     99\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 100\u001B[0;31m \u001B[0mtrain_transformer\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"Gemma_2B\"\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtrain_dataset\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mtraining_batch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0meval_batch_size\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m8\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlearning_rate\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m2e-5\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnum_train_epochs\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m3\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mweight_decay\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;36m0.01\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mdisable_tqdm\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mFalse\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mlora_config\u001B[0m\u001B[0;34m=\u001B[0m \u001B[0mlora_conf\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m<ipython-input-3-e54ef2bf7284>\u001B[0m in \u001B[0;36mtrain_transformer\u001B[0;34m(model1, dataset, training_batch_size, eval_batch_size, learning_rate, num_train_epochs, weight_decay, disable_tqdm, lora_config)\u001B[0m\n\u001B[1;32m    102\u001B[0m             \u001B[0mload_in_8bit\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mTrue\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    103\u001B[0m         )\n\u001B[0;32m--> 104\u001B[0;31m         model = AutoModelForSequenceClassification.from_pretrained(model_source, num_labels=2, token=access_token,\n\u001B[0m\u001B[1;32m    105\u001B[0m                                                                    quantization_config=bnb_config)\n\u001B[1;32m    106\u001B[0m         \u001B[0mmodel\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mprepare_model_for_kbit_training\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmodel\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m    561\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mkeys\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    562\u001B[0m             \u001B[0mmodel_class\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_get_model_class\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcls\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_model_mapping\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 563\u001B[0;31m             return model_class.from_pretrained(\n\u001B[0m\u001B[1;32m    564\u001B[0m                 \u001B[0mpretrained_model_name_or_path\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0mmodel_args\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mconfig\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mconfig\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mhub_kwargs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    565\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/modeling_utils.py\u001B[0m in \u001B[0;36mfrom_pretrained\u001B[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, *model_args, **kwargs)\u001B[0m\n\u001B[1;32m   3165\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3166\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mhf_quantizer\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 3167\u001B[0;31m             hf_quantizer.validate_environment(\n\u001B[0m\u001B[1;32m   3168\u001B[0m                 \u001B[0mtorch_dtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mtorch_dtype\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_tf\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfrom_tf\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfrom_flax\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfrom_flax\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdevice_map\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdevice_map\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   3169\u001B[0m             )\n",
      "\u001B[0;32m/usr/local/lib/python3.10/dist-packages/transformers/quantizers/quantizer_bnb_8bit.py\u001B[0m in \u001B[0;36mvalidate_environment\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m     60\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mvalidate_environment\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkwargs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     61\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0;34m(\u001B[0m\u001B[0mis_accelerate_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m \u001B[0;32mand\u001B[0m \u001B[0mis_bitsandbytes_available\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 62\u001B[0;31m             raise ImportError(\n\u001B[0m\u001B[1;32m     63\u001B[0m                 \u001B[0;34m\"Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` \"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     64\u001B[0m                 \u001B[0;34m\"and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: Using `bitsandbytes` 8-bit quantization requires Accelerate: `pip install accelerate` and the latest version of bitsandbytes: `pip install -i https://pypi.org/simple/ bitsandbytes`",
      "",
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001B[0;31m---------------------------------------------------------------------------\u001B[0m\n"
     ],
     "errorDetails": {
      "actions": [
       {
        "action": "open_url",
        "actionText": "Open Examples",
        "url": "/notebooks/snippets/importing_libraries.ipynb"
       }
      ]
     }
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from utils import make_prediction\n",
    "\n",
    "checkpoint = \"./checkpoint-3807\"\n",
    "predictions = make_prediction(MODELS,\"MISTRAL_7B\",test_dataframe,checkpoint)\n",
    "\n",
    "sample_submission = pd.read_csv(f'sample_submission.csv')\n",
    "sample_submission['target'] = predictions\n",
    "sample_submission.to_csv(f\"sample_submission_MISTRAL_changed_1.csv\", index=False)\n",
    "\n",
    "checkpoint = \"./checkpoint-7614\"\n",
    "predictions = make_prediction(MODELS, \"MISTRAL_7B\",test_dataframe,checkpoint)\n",
    "\n",
    "sample_submission = pd.read_csv(f'sample_submission.csv')\n",
    "sample_submission['target'] = predictions\n",
    "sample_submission.to_csv(f\"sample_submission_MISTRAL_changed_2.csv\", index=False)\n"
   ],
   "metadata": {
    "id": "rTwerVTIGk81",
    "ExecuteTime": {
     "start_time": "2024-05-08T18:20:44.696100Z"
    }
   },
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-08T18:20:55.897182Z",
     "start_time": "2024-05-08T18:20:55.871331Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  }
 ]
}
